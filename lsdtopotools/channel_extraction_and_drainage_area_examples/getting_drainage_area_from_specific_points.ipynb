{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4eee91c8",
   "metadata": {},
   "source": [
    "# Getting Drainage Area from specific points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86eeda37",
   "metadata": {},
   "source": [
    "Last updated by Simon M Mudd 11/04/2023"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f6a4db",
   "metadata": {},
   "source": [
    "In this notebook we will use an example where you have collected some channel characteristics in the field and we want to know the drainage area of the points. This will include the simplest possible example where all we have is the location of the points. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3144101b",
   "metadata": {
    "id": "QKy7_wKyMn9K"
   },
   "source": [
    "## Stuff we need to do if you are in colab (not required in the lsdtopotools pytools container)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec1cffe",
   "metadata": {
    "id": "qRWd-_X1vWDf"
   },
   "source": [
    "**If you are in the `docker_lsdtt_pytools` docker container, you do not need to do any of this. \n",
    "The following is for executing this code in the google colab environment only.**\n",
    "\n",
    "If you are in the docker container you can skip to the **Download some data** section. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49e1333",
   "metadata": {
    "id": "994da2f2"
   },
   "source": [
    "**If you are in the `docker_lsdtt_pytools` docker container, you do not need to do any of this. \n",
    "The following is for executing this code in the google colab environment only.**\n",
    "\n",
    "If you are in the docker container you can skip to the **First get data** section. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7652410a",
   "metadata": {
    "id": "Ck6Y73gLvbS8"
   },
   "source": [
    "First we install `lsdviztools`. \n",
    "This will take around a minute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761ebf9b",
   "metadata": {
    "id": "Pj2pwsxkvbS8"
   },
   "outputs": [],
   "source": [
    "!pip install lsdviztools &> /dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9ef5f9",
   "metadata": {
    "id": "Fq9UDKZrvbS8"
   },
   "source": [
    "Now we need to install lsdtopotools. We do this using something called `mamba`. Note that this version of `mamba` works for python 3.9 (which is what google colab currently uses).\n",
    "\n",
    "This step will take around 20 seconds. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9aec27",
   "metadata": {
    "id": "xhUkM9Q-9Cje"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "MINICONDA_INSTALLER_SCRIPT=Mambaforge-Linux-x86_64.sh\n",
    "MINICONDA_PREFIX=/usr/local\n",
    "wget https://github.com/conda-forge/miniforge/releases/latest/download/Mambaforge-Linux-x86_64.sh &> /dev/null\n",
    "chmod +x $MINICONDA_INSTALLER_SCRIPT\n",
    "./$MINICONDA_INSTALLER_SCRIPT -b -f -p $MINICONDA_PREFIX &> /dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89dfbc34",
   "metadata": {
    "id": "DXqY5s5T9bqL"
   },
   "source": [
    "Alternatively we can do this with condacolab, but this broke in March 2023 so will take some time to be fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc61bc1e",
   "metadata": {
    "id": "ea215034"
   },
   "outputs": [],
   "source": [
    "#!pip install -q condacolab\n",
    "#import condacolab\n",
    "#condacolab.install()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c550d0cb",
   "metadata": {
    "id": "9b8c3e17"
   },
   "source": [
    "Now use mamba to install `lsdtopotools`. \n",
    "This step takes a bit over a minute. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5285de3f",
   "metadata": {
    "id": "0568f44f"
   },
   "outputs": [],
   "source": [
    "!mamba install -y lsdtopotools &> /dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799c46a7",
   "metadata": {
    "id": "kNDf3rzQvJca"
   },
   "source": [
    "The next line tests to see if it worked. If you get some output asking for a parameter file then `lsdtopotools` is installed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eeacbcd",
   "metadata": {
    "id": "ee7ab7dc"
   },
   "outputs": [],
   "source": [
    "!lsdtt-basic-metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2146b6d9",
   "metadata": {
    "id": "2146b6d9"
   },
   "source": [
    "## First get data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec63078",
   "metadata": {},
   "source": [
    "Before we do anything, we need to import a few packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb29297c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lsdviztools.lsdbasemaptools as bmt\n",
    "from lsdviztools.lsdplottingtools import lsdmap_gdalio as gio\n",
    "import lsdviztools.lsdmapwrappers as lsdmw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035e7024",
   "metadata": {
    "id": "uMTDXJCUOSAG"
   },
   "source": [
    "Now we need to get some data to download. We are going to download data using the opentopography scraper that is included with `lsdviztools`. You will need to get an opentopography.org account and copy in your API key.\n",
    "\n",
    "You can sign up to an opentopography.org account here: https://portal.opentopography.org/myopentopo "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "167b7c80",
   "metadata": {},
   "source": [
    "Before I actually do anything I am going to set up some filenames:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c320b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataset_prefix = \"RioAguas\"\n",
    "source_name = \"COP30\"\n",
    "\n",
    "r_prefix = Dataset_prefix+\"_\"+source_name +\"_UTM\"\n",
    "w_prefix = Dataset_prefix+\"_\"+source_name +\"_UTM\"\n",
    "\n",
    "DataDirectory = \"./\"\n",
    "Base_file = r_prefix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1fac842",
   "metadata": {},
   "source": [
    "Now lets grab the data. If you want to do this yourself for a new area just choose your own lower lect and upper right coordinates of your site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05cfd96",
   "metadata": {
    "id": "m1yFGUrRInsF"
   },
   "outputs": [],
   "source": [
    "# YOU NEED TO PUT YOUR API KEY IN A FILE\n",
    "your_OT_api_key_file = \"my_OT_api_key.txt\"\n",
    "\n",
    "with open(your_OT_api_key_file, 'r') as file:\n",
    "    print(\"I am reading you OT API key from the file \"+your_OT_api_key_file)\n",
    "    api_key = file.read().rstrip()\n",
    "    print(\"Your api key starts with: \"+api_key[0:4])\n",
    "\n",
    "SB_DEM = bmt.ot_scraper(source = source_name,\n",
    "                        lower_left_coordinates = [36.97524478026287, -2.3631792251411805], \n",
    "                        upper_right_coordinates = [37.3200098350942, -1.7962073552766233],\n",
    "                        prefix = Dataset_prefix, \n",
    "                        api_key_file = your_OT_api_key_file)\n",
    "SB_DEM.print_parameters()\n",
    "SB_DEM.download_pythonic()\n",
    "DataDirectory = \"./\"\n",
    "Fname = Dataset_prefix+\"_\"+source_name+\".tif\"\n",
    "gio.convert4lsdtt(DataDirectory,Fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32535c8d",
   "metadata": {
    "id": "32535c8d"
   },
   "source": [
    "## Look at the hillshade"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82861e28",
   "metadata": {
    "id": "82861e28"
   },
   "source": [
    "Right, lets see what this place looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c6d4f1",
   "metadata": {
    "id": "52c6d4f1"
   },
   "outputs": [],
   "source": [
    "lsdtt_parameters = {\"write_hillshade\" : \"true\"}\n",
    "lsdtt_drive = lsdmw.lsdtt_driver(read_prefix = r_prefix,\n",
    "                                 write_prefix= w_prefix,\n",
    "                                 read_path = \"./\",\n",
    "                                 write_path = \"./\",\n",
    "                                 parameter_dictionary=lsdtt_parameters)\n",
    "lsdtt_drive.print_parameters()\n",
    "lsdtt_drive.run_lsdtt_command_line_tool()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9cbed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "Base_file = r_prefix\n",
    "DataDirectory = \"./\"\n",
    "this_img = lsdmw.SimpleHillshade(DataDirectory,Base_file,cmap=\"gist_earth\", save_fig=False, size_format=\"geomorphology\",dpi=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e48a872a",
   "metadata": {
    "id": "e48a872a"
   },
   "source": [
    "## Now get a single basin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f50f3c",
   "metadata": {
    "id": "a1f50f3c"
   },
   "source": [
    "I add a basin outlet into a pandas dataframe and then copy this to a file. \n",
    "The points below are obtained just by clicking in google maps and copying the resulting lat-long into the below code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cceedb5a",
   "metadata": {
    "id": "cceedb5a"
   },
   "outputs": [],
   "source": [
    "# Import pandas library\n",
    "import pandas as pd\n",
    "\n",
    "data = [ [37.15674383710805, -1.9049454817508027]]\n",
    "\n",
    "# Create the pandas DataFrame\n",
    "df = pd.DataFrame(data, columns = ['latitude', 'longitude'])\n",
    "\n",
    "df.to_csv(\"basin_outlets.csv\",index=False)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3359bbb3",
   "metadata": {
    "id": "3359bbb3"
   },
   "source": [
    "We can use the linux `cat` command to make sure the file is what we expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca33f67",
   "metadata": {
    "id": "5ca33f67"
   },
   "outputs": [],
   "source": [
    "!cat basin_outlets.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7775d95e",
   "metadata": {
    "id": "7775d95e"
   },
   "source": [
    "Now lets use *lsdtopotools* to get the basins. We first need to import the `lsdmapwrappers` module, and then run the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72cac9aa",
   "metadata": {
    "id": "72cac9aa"
   },
   "outputs": [],
   "source": [
    "import lsdviztools.lsdmapwrappers as lsdmw\n",
    "\n",
    "## Get the basins\n",
    "lsdtt_parameters = {\"print_basin_raster\" : \"true\",\n",
    "                    \"write_hillshade\" : \"true\",\n",
    "                    \"get_basins_from_outlets\" : \"true\",\n",
    "                    \"basin_outlet_csv\" : \"basin_outlets.csv\"}\n",
    "lsdtt_drive = lsdmw.lsdtt_driver(command_line_tool = \"lsdtt-chi-mapping\", \n",
    "                                 read_prefix = r_prefix,\n",
    "                                 write_prefix= w_prefix,\n",
    "                                 read_path = \"./\",\n",
    "                                 write_path = \"./\",\n",
    "                                 parameter_dictionary=lsdtt_parameters)\n",
    "lsdtt_drive.print_parameters()\n",
    "lsdtt_drive.run_lsdtt_command_line_tool()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "BSI6wubWo_N8",
   "metadata": {
    "id": "BSI6wubWo_N8"
   },
   "source": [
    "Now we can print the map with an lsdviztools call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b977fa23",
   "metadata": {
    "id": "b977fa23",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# uncomment this for debugging\n",
    "#import lsdviztools.lsdmapwrappers as lsdmw\n",
    "DataDirectory = \"./\"\n",
    "Base_file = r_prefix\n",
    "\n",
    "#%%capture\n",
    "basins_img = lsdmw.PrintBasins_Complex(DataDirectory,Base_file,cmap=\"gist_earth\", \n",
    "                             size_format=\"geomorphology\",dpi=600, save_fig = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c2ede4",
   "metadata": {
    "id": "62c2ede4"
   },
   "outputs": [],
   "source": [
    "print(basins_img)\n",
    "from IPython.display import display, Image\n",
    "display(Image(filename=basins_img, width=800))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b10437",
   "metadata": {},
   "source": [
    "We can get all the channels out of this basin with a call to `lsdtt-chi-mapping` (alhtough you can allso do this with `lsdtt-basic-metrics`. \n",
    "To get channels with various data elements such as the area, flow distance, elevation, and chi coordinate (if you don't know what that is, don't worry about it now), you use the keyword: `\"print_chi_data_maps\" : \"true\"`. \n",
    "\n",
    "We can also control the extent of the drainage network. Drainage extraction works by computing the flow direction of every pixel by seeing which of its eight neighbours is lowest. It then can count all the pixels contributing to a given pixel. Channels begin where they exceed a threshold number of contributing pixels. \n",
    "\n",
    "The default number of contributing pixels to being a channel is 1000, but you can use less with the keyword: `\"threshold_contributing_pixels\" : \"500\"` (you can of course change the number to whatever you want). \n",
    "\n",
    "We also want some additional information: the channel gradients and how they vary with drainage area. To get this we add `\"print_slope_area_data\" : \"true\"`. In addition we need to determine over which interval slopes are calculated. We calculate them over a fixed vertical interval: `\"SA_vertical_interval\" : \"10\"` so a new slope measurement is recorded whever the channel has fallen by 10m. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531106ec",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Get the basins and the channel profile\n",
    "lsdtt_parameters = {\"print_chi_data_maps\" : \"true\",\n",
    "                    \"get_basins_from_outlets\" : \"true\",\n",
    "                    \"basin_outlet_csv\" : \"basin_outlets.csv\",\n",
    "                    \"threshold_contributing_pixels\" : \"500\",\n",
    "                    \"SA_vertical_interval\" : \"10\",\n",
    "                    \"print_slope_area_data\" : \"true\"}\n",
    "r_prefix = Dataset_prefix+\"_\"+source_name +\"_UTM\"\n",
    "w_prefix = Dataset_prefix+\"_\"+source_name +\"_UTM\"\n",
    "lsdtt_drive = lsdmw.lsdtt_driver(command_line_tool = \"lsdtt-chi-mapping\", \n",
    "                                 read_prefix = r_prefix,\n",
    "                                 write_prefix= w_prefix,\n",
    "                                 read_path = \"./\",\n",
    "                                 write_path = \"./\",\n",
    "                                 parameter_dictionary=lsdtt_parameters)\n",
    "lsdtt_drive.print_parameters()\n",
    "lsdtt_drive.run_lsdtt_command_line_tool()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f6d13a",
   "metadata": {},
   "source": [
    "Lets see where the channels are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4118b1cc",
   "metadata": {
    "id": "b977fa23",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "this_chan_img = lsdmw.PrintChannelsAndBasins(DataDirectory,Base_file,\n",
    "                                       add_basin_labels = True, cmap = \"jet\", \n",
    "                                       size_format = \"ESURF\", fig_format = \"png\", \n",
    "                                       dpi = 300, save_fig = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3a37a5",
   "metadata": {
    "id": "62c2ede4",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "print(this_chan_img)\n",
    "from IPython.display import display, Image\n",
    "display(Image(filename=this_chan_img, width=800))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df58cff4",
   "metadata": {},
   "source": [
    "## Looking at gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3da6d66",
   "metadata": {},
   "source": [
    "The channel gradients are in `RioAguas_COP30_UTM_SAvertical.csv`, we can see it if we look for the files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f4fd88",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!ls *.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf3a6b6",
   "metadata": {},
   "source": [
    "This is what the data looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557c7953",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pandas library\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"RioAguas_COP30_UTM_SAvertical.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa271c39",
   "metadata": {},
   "source": [
    "## Import some points (that you record with a GPS) and combine with other data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yCOmerUe2LOr",
   "metadata": {
    "id": "yCOmerUe2LOr"
   },
   "source": [
    "Now we will import a dataset of points and combine it with other data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7oV41PzS17OD",
   "metadata": {
    "id": "7oV41PzS17OD"
   },
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from scipy.spatial import cKDTree\n",
    "from shapely.geometry import Point"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "iBzjaPCw2SwD",
   "metadata": {
    "id": "iBzjaPCw2SwD"
   },
   "source": [
    "We have two datasets. One is the channel data and the other is the site locations. This second dataset could be any set of points.\n",
    "\n",
    "We will, in the next step, merge these datasets based on the nearest neighbour to one of the set of points (i.e., mapping channel data to the nearest site).\n",
    "\n",
    "For this to work, the two datasets must be in the same coordinate reference system. For this example it is not really a problem because both datasets have coordinates in a global reference frame with the code EPSG:4326. In the example below, we use .crs to define the coordinate reference system. \n",
    "\n",
    "However, sometimes you might have a data set with another coordinate system (for example British National Grid, which is EPSG:27700, or UTM, which is EPSG:326XX where XX is the UTM zone) so you would need to change the corresponding EPSG code. You can look up the EPSG code for a coordinate system with a google search. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155f309c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls *.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d319ff99",
   "metadata": {},
   "source": [
    "We have a few csv datasets here. The ones you can cuse for this purpose are `RioAguas_COP30_UTM_SAvertical.csv` (which has slopes) and `RioAguas_COP30_UTM_chi_data_map.csv` (which does not have slopes). In the example below I use the one with slopes. \n",
    "\n",
    "Note that this is on a 30m DEM, so if you have collected data in the field of channel gradients with a laser or stadia rod+hand level, the field collected gradients will likeley be more accurate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "XB737H552PAz",
   "metadata": {
    "id": "XB737H552PAz"
   },
   "outputs": [],
   "source": [
    "# Load the channel data\n",
    "dfA = pd.read_csv(\"RioAguas_COP30_UTM_SAvertical.csv\")\n",
    "# Convert to a geopandas dataframe\n",
    "gdfA = gpd.GeoDataFrame(\n",
    "    dfA, geometry=gpd.points_from_xy(dfA.longitude, dfA.latitude))\n",
    "# We have to tell the geopandas data what geographic system we are in by using something called an EPSG code. \n",
    "# All major geographic projection and transformation system have this code. \n",
    "gdfA.crs = \"EPSG:4326\" \n",
    "\n",
    "\n",
    "# Load the width data\n",
    "dfB = pd.read_csv(\"Spain_2023_Grid_References.csv\")\n",
    "gdfB = gpd.GeoDataFrame(\n",
    "    dfB, geometry=gpd.points_from_xy(dfB.Easting, dfB.Northing))\n",
    "# We have to tell the geopandas data what geographic system we are in by using something called an EPSG code. \n",
    "# All major geographic projection and transformation system have this code. \n",
    "gdfB.crs = \"EPSG:32630\" \n",
    "\n",
    "# IMPORTANT: we convert one of the datasets to the coordinate reference system of the other\n",
    "gdfC = gdfB.to_crs(4326)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bkZBjRNC3z_N",
   "metadata": {
    "id": "bkZBjRNC3z_N"
   },
   "source": [
    "I now need to add a function for combining datasets. **You don't need to change anything in this function.** The first dataframe keeps its data elements and adds properties from the nearest neighbour that are closest to the points in the first dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eHz9vpmn37Xe",
   "metadata": {
    "id": "eHz9vpmn37Xe"
   },
   "outputs": [],
   "source": [
    "def ckdnearest(gdA, gdB):\n",
    "\n",
    "    nA = np.array(list(gdA.geometry.apply(lambda x: (x.x, x.y))))\n",
    "    nB = np.array(list(gdB.geometry.apply(lambda x: (x.x, x.y))))\n",
    "    btree = cKDTree(nB)\n",
    "    dist, idx = btree.query(nA, k=1)\n",
    "    gdB_nearest = gdB.iloc[idx].drop(columns=\"geometry\").reset_index(drop=True)\n",
    "    gdf = pd.concat(\n",
    "        [\n",
    "            gdA.reset_index(drop=True),\n",
    "            gdB_nearest,\n",
    "            pd.Series(dist, name='dist')\n",
    "        ], \n",
    "        axis=1)\n",
    "\n",
    "    return gdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mik7_Rzu3-nG",
   "metadata": {
    "id": "mik7_Rzu3-nG"
   },
   "source": [
    "Now we merge the two files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "MFsNj1tK39Tv",
   "metadata": {
    "id": "MFsNj1tK39Tv"
   },
   "outputs": [],
   "source": [
    "new_gdp = ckdnearest(gdfC, gdfA)\n",
    "new_gdp.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "NOOIFQwQ4J3v",
   "metadata": {
    "id": "NOOIFQwQ4J3v"
   },
   "source": [
    "Super! Now we can print this new dataset to a file using the .to_csv function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "-UZICSmx4CIW",
   "metadata": {
    "id": "-UZICSmx4CIW"
   },
   "outputs": [],
   "source": [
    "new_gdp.to_csv(\"updated_spain_site_infomation.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
